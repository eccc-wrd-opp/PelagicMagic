% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hotspot_functions.R
\name{get_hotspots}
\alias{get_hotspots}
\title{Identify hot spots from aerial census data}
\usage{
get_hotspots(
  obs,
  regions = NULL,
  coast,
  time_group = c("\%Y", "\%m", "\%d")[1],
  res = 250,
  h = 250,
  sum_stat = c("max", "mean", "median")[1],
  lev = c(1, 10, 100, 1000, 10000),
  buffer_points = 10000,
  area_unit = c("km", "m")[1],
  donuts = T,
  summaries = T,
  output = c("polygons", "summary_rast", "all_rast")[1]
)
}
\arguments{
\item{obs}{An sf points object of survey observations with one row per individual, must include a column named 'date' of class Date or POSIX.}

\item{regions}{An optional sf polygon object to split flocks into regions for faster analysis.}

\item{coast}{An sf polygons object of land areas to mask from utilization distribution.}

\item{time_group}{A date abbreviation to indicate how surveys should be grouped for analysis. One of \%Y, \%m, \%d.}

\item{res}{Spatial raster resolution for utilization distribution in meters.}

\item{h}{Smoothing parameter for utliization distribution in meters.}

\item{sum_stat}{Statistic for summarizing distribution across survey events, one of 'max', 'mean', or 'median'.}

\item{lev}{List of breakpoints for summarizing the raster distribution into polygons. Values below first level will be dropped.}

\item{buffer_points}{Distance to buffer points when creating study area polygon if regions argument is NULL.}

\item{area_unit}{Unit of measurement for area calculations, one of 'km' or 'm'.}

\item{donuts}{Logical. Should higher level polygons be erased from lower level polygons to create donuts.}

\item{summaries}{Logical. Should summaries of bird observations within each polygon be returned.}

\item{output}{Should the function return summary polygons, summary rasters or all weighted utlization rasters, one of 'polygons', 'summary_rast', or 'all_rast'.}
}
\value{
An sf POLYGONS object showing hotspots
}
\description{
Identify hot spots from aerial census data
}
\details{
This function calculates a utilization distribution (UD) for each unique level in time_group.
The observations can be broken up into regions to reduce the size of the raster on which the UD is calculated.
Each raster is then weighted by the sum of all observations within the time_group+region combination. Rasters
are mosaiced together and summarized using the function option provided in sum_stat.

Continuous outputs from the rasters can be converted into polygons by classifying
observations according to the breakpoints in lev. This creates a smoothed polygon
representing areas with weighted utlization distribution values within each class.
Polygons can be converted to donuts with higher levels of expected distribution
removed from lower level polygons. Summaries are provided for the size of the polygons, and the
mean and max counts observed within polygons and the number of surveys conducted of that area.
}
\examples{
# format data
d <- coei
d <- sf::st_as_sf(d, coords = c('longitude', 'latitude'), crs = 4326)
d <- sf::st_transform(d,
                      crs = '+proj=laea +lat_0=50 +lon_0=-65 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs')

# get coast layer
coast <- rnaturalearth::ne_countries(scale = 10, return = 'sf')
coast <- sf::st_transform(coast, crs = sf::st_crs(d))
coast_crop <- sf::st_crop(coast, sf::st_buffer(d, dist = 10000))

# generate flocks from observation data
f <- get_flocks(dat = d,
                flock_spacing = 10,
                search_dist = 100,
                np = 100,
                coast = coast_crop)

reg <- f |>
  sf::st_buffer(dist = 10000) |>
  sf::st_bbox() |>
  sf::st_as_sfc() |>
  sf::st_as_sf()

# generate hoptpots
hs <- get_hotspots(
  obs = f,
  regions = reg,
  coast = coast_crop,
  time_group = c('\%Y', '\%m', '\%d')[1],
  res = 250,
  h = 250,
  sum_stat = c('max','mean','median')[1],
  lev = c(1, 10, 100, 1000, 10000),
  buffer_points = 10000,
  area_unit = c('km','m')[1],
  donuts = T,
  summaries = T
)
hs

}
